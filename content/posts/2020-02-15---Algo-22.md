---
title: Comparing Algorithms with Big-O notation
date: '2020-02-15'
time: '☕️'
template: "post"
draft: false
slug: "Algo-22"
category: "Algorithms"
tags:
  - "Algorithms"
description: "Let's dive into the Big-O notation to compare algorithms efficiency with Jose Portilla and Andrei Neagoie."
socialImage: "/media/big-o.png"
---

This article was done using my notes from two authors: Jose Portilla and Andrei Neagoie.

1. Portilla J., (2019), Python for Data Structures, Algorithms and Interviews
2. Neagoie  A., (2020), Master the Coding Interview: Data Structures + Algorithms

## Why analyze algorithms ?

Algorithms can be understood as a procedure or formula for solving a problem. Some problems are so famous that the algorithms have given names, or the procedures being used within the algorithm has also a given name.

How would compare algorithms ? How would you now which one is better ? And so, what is "better" ?

## Space and Memory

Comparing algorithms, the "better" has to be understood in termes of space and memory. We can objectively compare algorithms through the amount of space they take in memory or we can also compare how much time it takes for each function to run. To make it short, the best algorithm would be the most effecient in terms of space and memory. The one taking the least time and the least memory.

In the case of space, we would speak of space complexity. In the case of memory, we would speak of time complexity.

To do so, we make use of the Big-O notation.

## Big-O Notation for time complexity

Big-O notation describes how quickly runtime will grow relative to the input as the input get arbitrarily large. 

Using Big-O notation, we want:

1. to compare how quickly runtime will grow, not compare exact runtimes, since those can vary depending on hardware.

2. to compare for a variety of input sizes, we are only concerned with runtime grow relative to the input. This is why we use the **n** for notation.

3. to only worry about terms that will grow the fastest as **n** gets arbitrarily large.

### Runtimes of common Big-O functions

| Big-O         | Name          |
| ------------- |:-------------:|
| 1             | Constant      |
| log(n)        | Logarithmic   |
| n             | Linear        |
| nlog(n)       | Log Linear    |
| n^2           | Quadratic     |
| n^3           | Cubic         |
| 2^n           | Exponential   |

As a thing to remember, you really want to avoid above nlog(n) functions.

![Big O notation](/media/big-o.png)



### Big-Os to remember for time complexity

**O(1)** is Constant, to be found in functions without loops.

**O(log N)** is Logarithmic, to be found in searching algorithms if they are sorted (Binary Search).

**O(n)** is Linear, to be found in functions using for loops or while loops through **n** items.

**O(n log(n))** is Log Liniear, to be found in sorting operations.

**O(n^2)** is Quadratic, to be found in functions where every element in a collection needs to be compared to ever other element. This is the case of two nested loops.

**O(2^n)** is Exponential, to be found in function with recursive algorithms that solves a problem of size **N**.

**O(n!)** is Factorial, to be found in function adding a loop for every element. 


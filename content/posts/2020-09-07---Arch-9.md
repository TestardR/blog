---
title: Cache to scale access to data
date: '2020-09-07'
time: '☕️'
template: 'post'
draft: false
slug: 'Arch-9'
category: 'Architecture'
description: "Let's take a look at scaling access to data using cache with Kate Matsudaira"
---

This article was done using my notes from Kate Matsudaira (2013).

<sub>Kate Matsudaira, (2013), "Scalable Web Architecture and Distributed Systems". In: Amy Brown & Greg Wilson (2013), "The Architecture of Open Source Applications, Volume II: Structure, Scale, and a Few More Fearless Hacks.</sub>

## Building blocks of fast and scalable data access

As applications grow, there are two main challenges: scaling access to the app server and to the database.

Let’s assume you have many terabytes (TB) of data and you want to allow users to access small portions of that data at random. You might consider using memory but it can be very costly to load TBs of data into it. Furthermore, reading from disk is many times slower than from memory. Moreover, even with unique IDs, solving the problem of knowing where to find that little bit of data can be an arduous task.

Cache is one of the options you have to make this issue easier to deal with.

### Cache

Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again. A cache is like short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items. Caches can exist at all levels in architecture, but are often found at the level nearest to the front end, where they are implemented to return data quickly without taxing downstream levels.

#### How can a cache be used to make your data access faster in our API example?

One option is to insert a cache on your request layer node. Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local, cached data if it exists. If it is not in the cache, the request node will query the data from disk.

#### What happens when you expand this to many nodes?

If the request layer is expanded to multiple nodes, it’s still quite possible to have each node host its own cache. However, if your load balancer randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. Two choices for overcoming this hurdle are global caches and distributed caches.

### Global Cache

A global cache is just as it sounds: all the nodes use the same single cache space. This involves
adding a server, or file store of some sort, faster than your original store and accessible by all the request layer nodes. Each of the request nodes queries the cache in the same way it would a local one.

There are two common forms of global caches.

1. When a cached response is not found in the cache, the cache itself becomes responsible for retrieving the missing piece of data from the underlying store.
2. It is the responsibility of request nodes to retrieve any data that is not found in the cache.

### Distributed Cache

In a distributed cache, each of its nodes own part of the cached data. Typically the cache is divided up using a consistent hashing function, such that if a request node is looking for a certain piece of data it can quickly know where to look within the distributed cache to determine if that data is available.

### Memcached

One example of a popular open source cache is [Memcached](https://memcached.org/) (which can work both as a local
cache and distributed cache). Memcached is used in many large web sites, and even though it can be very powerful, it is simply an in-memory key value store, optimized for arbitrary data storage and fast lookups (O(1)).

---
title: "Caching"
date: "2021-02-07"
time: "☕️"
template: "post"
draft: false
slug: "Arch-20"
category: "Architecture"
description: "Let's take a look at Caching with Design Gurus"
---

This article was done using my notes from:

<sub>Design Gurus, (2021), [Caching](https://www.educative.io/courses/grokking-the-system-design-interview/3j6NnJrpp5p). educative.</sub></br>

## Definition

A cache is like short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items. Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again. 

Caches can exist at all levels in architecture, but are often found at the level nearest to the front end where they are implemented to return data quickly without taxing downstream levels.

## Application server cache

Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local cached data if it exists. If it is not in the cache, the requesting node will query the data from disk.

## Content Distribution Network (CDN)

CDNs are a kind of cache that comes into play for sites serving large amounts of static media. In a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that content if it has it locally available. If it isn’t available, the CDN will query the back-end servers for the file, cache it locally, and serve it to the requesting user.

## Cache Invalidation

While caching is fantastic, it does require some maintenance for keeping cache coherent with the source of truth (e.g., database). If the data is modified in the database, it should be invalidated in the cache; if not, this can cause inconsistent application behavior. 

Solving this problem is known as cache invalidation; there are three main schemes that are used:

1. **Write-through cache**: Under this scheme, data is written into the cache and the corresponding database at the same time. This scheme has the disadvantage of hihgher latency for write operations.

2. **Write-around cache**: This technique is similar to write through cache, but data is written directly to permanent storage, bypassing the cache. This schema has the disadvantage that a read request for recently written data will create a “cache miss” and must be read from slower back-end storage and experience higher latency.

3. **Write-back cache**: Under this scheme, data is written to cache alone and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low latency and high throughput for write-intensive applications, however, this speed comes with the risk of data loss in case of a crash.

## Cache eviction policies

1. First In First Out (FIFO)
2. Last In First Out (LIFO)
3. Least Recently Used (LRU)
4. Most Recently Used (MRU)
5. Least Frequently Used (LFU)
6. Random Replacement (RR)


